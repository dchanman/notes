{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## AMD vs Intel\n",
    "* In 2006, AMD had a larger market share (+5%). Suddenly, in 2007, Intel overtook the lead (+14%)\n",
    "* Different architectures provide optimal solutions to certain problems\n",
    "    * AMD for bitcoin mining\n",
    "    * NVDIA for gaming\n",
    "\n",
    "## Introduction\n",
    "Everyone uses MIPS64 instruction set, so it's worth knowing it.\n",
    "\n",
    "### Task of the Computer Designer\n",
    "* Coordinate many levels of abstraction\n",
    "* Under a rapidly changing set of forces\n",
    "* Design, measure, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Principles\n",
    "\n",
    "### *Make the Common Case Fast*\n",
    "* eg. ketchup bottle is upside down so that ketchup is always ready\n",
    "\n",
    "* Compilers (eg. GCC) will actually output different instructions depending on their era. The different combinations are a result of the trends of the current hardware\n",
    "\n",
    "> Two engineers argue. One wants a FP unit that improves division 40x. Another wants a general FP unit that improves everything by 1.5x. Which is better?\n",
    "* Application dependent\n",
    "\n",
    "### *Take Advantage of Parallelism*\n",
    "* **TLP**: Thread level: independent programs run on different processors\n",
    "* **DLP**: Data level: multiple instances of the same program on different input data\n",
    "* **ILP**: Instructions level: different instructions may be run simultaneously\n",
    "* Different parts in a digital circuit can be parallel during a clock cycle (eg. processes in VHDL architecture, pipelining)\n",
    "\n",
    "### *Principle of Locality*\n",
    "Real programs are not completely random; programs have structure and purpose. Many are quite predictable.\n",
    "\n",
    "* eg. programs spend 90% of the time executing 10% of code\n",
    "\n",
    "### *Bang for Buck (!/\\$)*\n",
    "\n",
    "### *Amdahl's Law*\n",
    "Gene Amdahl argued that it was very important to focus on a single processor/core since there is unparallelizable code that will always be the bottleneck.\n",
    "\n",
    "$$ Speedup = \\frac{ExTime_{old}}{ExTime_{new}} = \\frac{1}{(1 - Frac_{enhanced}) + \\frac{Frac_{enh}}{Speedup_{enh}}} $$\n",
    "\n",
    "---\n",
    "\n",
    "# Example: Floating Point (FP) Square Root (FPQSR)\n",
    "\n",
    "* 20% of ExTime due to FPSQR\n",
    "* 50% of ExTime due to **all** FP operations\n",
    "\n",
    "Two options:\n",
    "* Speedup FPSQR by factor of 10\n",
    "* Speedup all FP by a factor of 1.6\n",
    "\n",
    "Result:\n",
    "* FPSQR = 1.22 overall speedup\n",
    "* FP = 1.23 overall speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Tuesday 13 September*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "\n",
    "> Does simulation accuracy limit innovation?\n",
    "\n",
    "If simulations were cycle accurate, we would not be able to measure results. It would take too long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of Performance\n",
    "\n",
    "Stack | Care\n",
    "---|---\n",
    "Application | Answers/month, ops/sec\n",
    "Programming language | programs/time\n",
    "Compiler |\n",
    "**ISA** | MIPS/MFLOP/s (millions of instr./s, milions of floating point ops/sec)\n",
    "Datapath control | gigabytes per second\n",
    "Function units, Transistors, wires/pins | cycles per second\n",
    "\n",
    "* Perhaps as an architect, we would make our chip optimized for certain instructions to be faster. Or we prioritize MIPS over MFLOPS.\n",
    "\n",
    "## Definitions\n",
    "* **Performance**: units of things per second. Bigger is better\n",
    "    * Notice there are exceptions. For example, Google might have a high macroscopic request/sec performance. However, each individual request may take too long. In this case, we may want a smaller performance.\n",
    "    * $\\text{Performance}(x) = \\frac{1}{executionTime(x)}$\n",
    "    * $speedup = n = \\frac{performance(x)}{performance(y)} = \\frac{exectime(x)}{exectime(y)}$\n",
    "    * Alternative marketing definitions:\n",
    "        * $perf = \\frac{instructions}{second}$\n",
    "        * $perf = FLOPS$\n",
    "        * $perf = GHz$\n",
    "        * In these cases, maybe something could be clocked faster. However, if it had bad architecture, it could be worse performing compared to slower chips\n",
    "        * Only when comparing two exact architectures may these definitions be interesting\n",
    "* **Benchmarks**: Programs which evaluate performance\n",
    "    * Real applications (eg. weather simulations)\n",
    "        * May take too long to run a full program\n",
    "    * Kernels\n",
    "        * Small key pieces from real programs (eg. linpack)\n",
    "        * Kernel may not be a perfect representative of a program\n",
    "    * Toy benchmarks\n",
    "        * Sieve of Eratosthenes, Puzzle, Quicksort\n",
    "        * Leave these for APSC160/CPSC260, most applications don't spend most of its time doing one specific algorithm\n",
    "    * Synthemtic Benchmarks\n",
    "        * Programs that no customer will ever run\n",
    "        * Poor for sales, since you're not actually building what somebody wants\n",
    "\n",
    "## Benchmark Suites\n",
    "A collection of applications used to measure performance of copmuter\n",
    "\n",
    "Different companies may have different benchmark suites because they're targeting specific goals\n",
    "* What do you want out of a computer?\n",
    "* What programs do you run most often?\n",
    "\n",
    "eg. [SPECint CPU2006](www.spec.org)\n",
    "* 12 applications (gzip, gcc, perl, + other more exotic ones)\n",
    "* Benchmarks update as software improves\n",
    "\n",
    "### Comparing and Summarizing Performance\n",
    " | Computer A | Computer B | Computer C\n",
    " ---|---|---\n",
    " Prog P1 | 1s | 10s | 20s\n",
    " Prog P2 | 1000s | 100s | 20s\n",
    " \n",
    " The fastest computer depends on which program matters for you. **No computer is *fastest overall*.**\n",
    " \n",
    " Typically, large enough companies will get a bunch of samples and run their own benchmarks (eg. Oracle)\n",
    " \n",
    " You can summarize performance using average execution time:\n",
    " $$Average Exec Time = \\frac{1}{n}\\sum_{i=1}^{n}Time_i$$\n",
    " \n",
    " What if you don't run programs 1 and 2 the same number of times?\n",
    " \n",
    " * Use weighted Execution Time\n",
    " \n",
    " $$Weighted Exec Time = \\sum{i=1}{n}Weight_i \\times Time_i$$\n",
    " \n",
    " * Use Geometric Mean\n",
    "     * Execution Time Ratio is the *speedup* for benchmark *i*\n",
    "     * This is the most common one that people use\n",
    " \n",
    " $$Geometric Mean = \\sqrt[n]{\\Pi_{i=1}^{n}ExecutionTimeRatio_i}$$\n",
    " \n",
    " $$\\frac{GeometricMean(X_1, X_2, \\ldots, X_n)}{GeometricMean(Y_1, Y_2, \\ldots, Y_n)} = GeometricMean(\\frac{X_1}{Y_1}, \\frac{X_2}{Y_2}, \\ldots, \\frac{X_n}{Y_n})$$\n",
    " \n",
    "### Example: Weighted Execution Time\n",
    "* The \"fastest\" computer is determined by the weighting of program mix\n",
    "\n",
    "### Example: Arithmetric Mean vs Geometric Mean\n",
    "* Arithmetic mean still allows you to \"cook\" the numbers to make a certain computer faster. The result depends on whichever \"base machine\" you use to base all speedups\n",
    "* Geometric mean will always pick the same machine regardless of the \"base machine\"\n",
    "    * Drawback: geo mean doesn't predict execution time\n",
    "    \n",
    "> *What are we comparing when we use \"Average Performance?\"*\n",
    "> * One computer versus another computer across a set of different programs\n",
    "\n",
    "### Harmonic Mean\n",
    "\n",
    "$$Harmonic Mean = \\frac{n}{\\sum\\frac{1}{ExecutionTimeRation_i}}$$\n",
    "\n",
    "* Gives more weight to smaller differences\n",
    "\n",
    "Mathematical relationship:\n",
    "$$harmonic \\le geometric \\le arithmetic$$\n",
    "\n",
    "### SPEC Benchmark Evolution\n",
    "* A suite of benchmarks\n",
    "* Changes of included programs reflect changes in computer usage at the time\n",
    "* Uses geometric mean speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Versus Price\n",
    "\n",
    "The stack of price\n",
    "\n",
    "Component | Percent of total cost\n",
    "---|---\n",
    "Average consumer discount | 28%\n",
    "Gross margin | 19%\n",
    "Direct costs | 10%\n",
    "Component costs | 47%\n",
    "\n",
    "Obviously this is for everything business.\n",
    "\n",
    "## Trends\n",
    "* Over time, it costs less to make a chip\n",
    "* You'll become more and more efficient and produce greater yield\n",
    "* The next generation will follow the same curve, albeit time delayed\n",
    "\n",
    "## Economic Impact\n",
    "* Cost leads to tradeoffs in design\n",
    "* Cost changes due to *learning curves*, where the yield curve improves\n",
    "* How does volume impact cost\n",
    "    * Reduces time needed to get down the learning curve\n",
    "    * Increase purchasing and manufacturing efficiency\n",
    "    * Amortizes development cost (decrease price)\n",
    "* Commodification leads to competition, which reduces price\n",
    "\n",
    "## Integrated Circuits Cost\n",
    "---\n",
    "\n",
    "$$ IC Cost = \\frac{ Die Cost + Testing Cost + Packaging Cost}{Final Test Yield}$$\n",
    "\n",
    "---\n",
    "\n",
    "$$ Die Cost = \\frac{Wafer Cost}{DiesPerWafer \\times DieYield}$$\n",
    "\n",
    "---\n",
    "\n",
    "$$ Dies Per Wafer = \\frac{\\pi(\\frac{WaferDiam}{2})^2}{DieArea} - \\frac{\\pi \\times WaferDiam}{\\sqrt{2 \\times DieArea}} - TestDie$$\n",
    "\n",
    "On each wafer, you will sacrifice several dies to make sure that the design works.\n",
    "\n",
    "---\n",
    "\n",
    "$$DieYield = WaferYield \\times (1 + \\frac{DefectDensity \\times DieArea}{a})^{-a}$$\n",
    "\n",
    "---\n",
    "\n",
    "Some manufacturers produce broken chips where some cores don't work. This is fine, they sell them as lower-grade processor chips. For example, a 4 core chip where 2 cores don't work are sold as 2 core chips.\n",
    "\n",
    "### Example Question\n",
    "\n",
    "$$ Testing Cost = \\frac{\\text{Cost of testing per hour} \\times \\text{Average die test time}}{\\text{Die yield}}$$\n",
    "\n",
    "> Determine cost if defect exist when density = $\\frac{0.3}{cm^2}$ vs $\\frac{1.0}{cm^2}$\n",
    "\n",
    "It's cheaper to use the larger density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power\n",
    "\n",
    "Power usage has effectively dominated the constraints for high performance\n",
    "\n",
    "## Dynamic Power\n",
    "\n",
    "**Dynamic Power** is the cost of switching transistors back and forth\n",
    "$$ Power = \\frac{1}{2} \\times CapacitiveLoad \\times V^2 \\times f$$\n",
    "\n",
    "Power is dissapated typically as heat\n",
    "\n",
    "For mobile devices, energy is a better metric. It's the same thing minus dependence on frequency. Consider lifting a weight, it doesn't change the amount of work you do depending on your rate of lift.\n",
    "\n",
    "$$Energy = CapacitiveLoad \\times V^2$$\n",
    "\n",
    "Capacitive load is a function of\n",
    "* the number of transistors connected to output (number being switched)\n",
    "* technology which determines capacitance of wires and transistors (capacitors per transistors)\n",
    "\n",
    "Dropping voltage helps both power and energy\n",
    "\n",
    "**For a fixed task, slowing clock rate reduces power but not energy**\n",
    "\n",
    "\n",
    "\n",
    "### Questions\n",
    "> A decrease of 15% in frequency allows 15% decrease in voltage. What is the overall decrease in power?\n",
    "\n",
    "---\n",
    "\n",
    "## Static Power\n",
    "\n",
    "**Static power** is caused by leakage current, which flows even when a transistor is off.\n",
    "\n",
    "$$Power = Current_{static} \\times V$$\n",
    "\n",
    "* Leakage increases in processors with smaller transistor sizes\n",
    "* Increasing number of transistors increases power, even if they're off\n",
    "* Goal for leakage is to get it as low as 25% overall, high performance aims for 40%\n",
    "* Very low power systems turn off inactive modules to control leakage loss\n",
    "\n",
    "### Technology scaling trends\n",
    "* We can still increase transistor density\n",
    "* Dynamic power prevents us from further increasing frequency\n",
    "* **Moore's Law**: There is an optimal cost per chip, there is an optimal size where you have an optimal number of transistors\n",
    "    * Really big chips will have certainty for defects\n",
    "    * Really small chips will require more testing\n",
    "    \n",
    "### Dennard Scaling\n",
    "* 2x more transistors requires more power\n",
    "* 1.4x faster transistors also requires more power\n",
    "* 0.7x capacitance improvement and 0.7x voltage improvement balanced the increased power\n",
    "* Basically this all came together to mean chip capabilities came for free for a long time\n",
    "\n",
    "**Post-Dennard Scaling**: Nowadays, leakage limits reduction in voltage improvement. This means that we no longer get free improvements; we pay for improvements with higher power consumption\n",
    "\n",
    "Restrictive geometry = 1.2x scaling at the same power\n",
    "\n",
    "### Latency Lags Bandwith (for the last few decades)\n",
    "* Rule of thumb: 2x bandwith ~ 1.2 - 1.4x latency\n",
    "* Processing limited by memory bandwidth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Set Architecture Design\n",
    "\n",
    "Up until this point, we've covered all of the constraints to processor design (eg. laws, power, demand, etc).\n",
    "\n",
    "Now we'll cover instruction set architecture design.\n",
    "\n",
    "## Instruction Set Architecture (ISA)\n",
    "* Low level SW interface to the computer\n",
    "* Language understood by computer\n",
    "* Any programming language must be translated into this language\n",
    "    * eg. ARM, MIPS, x86\n",
    "* Represents the features available to the programmer\n",
    "\n",
    "## Levels of Representation\n",
    "1. High level language\n",
    "2. Compiled to Assembly Language\n",
    "3. Assembled to Machine Language Program\n",
    "4. Interpreted as control signals\n",
    "\n",
    "## Fundamental Execution Cycle\n",
    "1. Fetch instruction\n",
    "2. Decode instruction\n",
    "3. Fetch oerand\n",
    "4. Execute\n",
    "5. Store result\n",
    "6. Next instruction\n",
    "\n",
    "> Does \"instruction fetch\" depend on the result of the last instruction?\n",
    "\n",
    "## MIPS ISA\n",
    "* All instructions the same size\n",
    "* Load-store architecture\n",
    "    * arithmetic is register-to-register\n",
    "    * separates computation from memory traffic\n",
    "* Divided in three categories\n",
    "    * high level for hardware reuse\n",
    "    \n",
    "* x86 specific instructions have translators into microops (MIPS-like)\n",
    "    * legacy support\n",
    "    * specific utility instructions (eg. loadstring)\n",
    "\n",
    "## CISC/RISC\n",
    "* RISC (reduced instruction set computer)\n",
    "* CISC (complex insturction set computer)\n",
    "\n",
    "## Example MIPS64 ALU Instructions\n",
    "```\n",
    "Regs[Rn]             Contents of register n\n",
    "Mem[Addr]            Contents at location Addr\n",
    "##                   Concatenate bits\n",
    "<-                   Assign RHS to LSH\n",
    "Superscript          Replicate a field\n",
    "Subscript            Selection of bit (most significant = 0)\n",
    "```\n",
    "\n",
    "In load/store instructions, indices are provided. This is for C stuff like `structs` and `arrays`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
